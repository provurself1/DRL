# Import necessary libraries
import numpy as np
import random
from itertools import permutations
import logging
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, Flatten
from keras.optimizers import Adam
from collections import deque

# Configure logging
logging.basicConfig(filename='training_log.txt', level=logging.INFO, format='%(asctime)s %(message)s')

# Defining hyperparameters
m = 5  # number of cities
t = 24  # number of hours
d = 7  # number of days
C = 5  # Per hour fuel and other costs
R = 9  # base revenue from a passenger
surge_factor = 2  # Surge pricing factor for peak hours

class CabDriver:
    def __init__(self, driver_id):
        """Initialize state and define action space and state space"""
        self.driver_id = driver_id
        self.action_space = list(permutations([i for i in range(m)], 2)) + [(0, 0)]  # All permutations + no action
        self.state_space = [[x, y, z] for x in range(m) for y in range(t) for z in range(d)]
        self.state_init = random.choice(self.state_space)
        self.reset()

    def reset(self):
        """Reset the driver's state"""
        self.current_state = self.state_init
        return self.current_state

    def dynamic_reward(self, hour):
        """Calculate dynamic reward based on the hour of the day"""
        if 7 <= hour < 10 or 17 <= hour < 20:  # Peak hours
            return R * surge_factor
        else:
            return R

    def step(self, action):
        """Take an action and return the new state and reward"""
        current_city, current_hour, current_day = self.current_state

        # Action is a tuple (pickup_city, dropoff_city)
        pickup_city, dropoff_city = action

        # Reward is based on whether the agent makes a successful pickup and dropoff
        reward = 0
        if pickup_city != dropoff_city:  # Successful pickup/dropoff
            reward = self.dynamic_reward(current_hour)

        # Simulate state transition:
        # Assume the agent can travel to the dropoff city and update the state
        new_hour = (current_hour + random.randint(1, 2)) % t  # Randomly increments hour (travel time)
        new_day = (current_day + (current_hour + 1) // t) % d  # Increment day if needed
        new_state = (dropoff_city, new_hour, new_day)

        # Update the current state to the new state for the next step
        self.current_state = new_state
        return new_state, reward


class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=2000)
        self.gamma = 0.95  # discount rate
        self.epsilon = 1.0  # exploration rate
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.model = self._build_model()

    def _build_model(self):
        model = Sequential()
        model.add(Flatten(input_shape=(self.state_size,)))
        model.add(Dense(24, activation='relu'))
        model.add(Dense(24, activation='relu'))
        model.add(Dense(self.action_size, activation='linear'))
        model.compile(loss='mse', optimizer=Adam(lr=0.001))
        return model

    def get_action(self, state):
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)
        act_values = self.model.predict(state)
        return np.argmax(act_values[0])  # returns action

    def remember(self, state, action, reward, next_state):
        self.memory.append((state, action, reward, next_state))

    def train_model(self, batch_size):
        if len(self.memory) < batch_size:
            return
        minibatch = random.sample(self.memory, batch_size)
        for state, action, reward, next_state in minibatch:
            target = reward
            if next_state is not None:
                target += self.gamma * np.amax(self.model.predict(next_state)[0])
            target_f = self.model.predict(state)
            target_f[0][action] = target
            self.model.fit(state, target_f, epochs=1, verbose=0)


class MultiDQNAgent:
    def __init__(self, state_size, action_size, num_agents):
        self.state_size = state_size
        self.action_size = action_size
        self.agents = [DQNAgent(state_size, action_size) for _ in range(num_agents)]

    def act(self, state, agent_id):
        """Choose action for a specific agent"""
        return self.agents[agent_id].get_action(state)

    def train(self, agent_id, experience_batch):
        """Train a specific agent"""
        self.agents[agent_id].train_model(experience_batch)

    def reset_agents(self):
        """Reset all agents"""
        for agent in self.agents:
            agent.reset()


# Setup for training
num_agents = 3
num_episodes = 1000
max_steps = 100
state_size = 3  # Example: (city, hour, day)
action_size = len(list(permutations(range(m), 2))) + 1  # All permutations + no action
agents = MultiDQNAgent(state_size, action_size, num_agents)

# Create environment instances for each agent
environment = [CabDriver(driver_id=i) for i in range(num_agents)]

# Training loop
total_rewards = []
epsilon_values = []

for episode in range(num_episodes):
    states = [env.reset() for env in environment]
    total_reward = 0
    for time_step in range(max_steps):
        for agent_id in range(num_agents):
            action = agents.act(np.array([states[agent_id]]), agent_id)
            new_state, reward = environment[agent_id].step(action)
            agents.train(agent_id, (np.array([states[agent_id]]), action, reward, np.array([new_state])))
            states[agent_id] = new_state
            total_reward += reward

    # Decay epsilon after each episode
    if agents.agents[0].epsilon > agents.agents[0].epsilon_min:
        agents.agents[0].epsilon *= agents.agents[0].epsilon_decay

    # Log the episode results
    logging.info(f"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}, Epsilon: {agents.agents[0].epsilon:.4f}")
    total_rewards.append(total_reward)
    epsilon_values.append(agents.agents[0].epsilon)

    # Optionally, print episode results
    print(f"Episode {episode + 1}/{num_episodes} completed with total reward: {total_reward}.")

# Save the models after training
for agent_id in range(num_agents):
    agents.agents[agent_id].model.save(f"dqn_agent_{agent_id}.h5")

# Visualization of training results
episode_numbers = list(range(1, num_episodes + 1))

# Plot total rewards per episode
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.plot(episode_numbers, total_rewards, label='Total Reward', color='blue')
plt.title('Total Reward per Episode')
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.grid()
plt.legend()

# Plot epsilon decay
plt.subplot(1, 2, 2)
plt.plot(episode_numbers, epsilon_values, label='Epsilon', color='orange')
plt.title('Epsilon Decay Over Episodes')
plt.xlabel('Episode')
plt.ylabel('Epsilon Value')
plt.grid()
plt.legend()

plt.tight_layout()
plt.show()
